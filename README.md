# Cross-cluster Connectivity

Multi-cluster DNS for [Cluster API](https://cluster-api.sigs.k8s.io/)


## Status

This project is in the middle of a reboot, to simplify the operational model
and utilize a Cluster API control plane.  We're keeping the old code on the `v1`
branch.  New development is happening in PRs against `main`.

A high-level tracking issue is [#38](https://github.com/vmware-tanzu/cross-cluster-connectivity/issues/38).


## What?

This project aims to enables a Kubernetes service in one cluster to be
discovered and used by a client pod in a different cluster, while putting
minimal requirements on the clusters or their networking environment.


## Why?

There are various ways to share Kubernetes Services across clusters. Each has
its limitations, often due to assumptions made about the infrastructure network,
DNS, or tenancy model of the clusters.

This project aims to provide cross-cluster service discovery and connectivity
even when:

1. Service type:LoadBalancer is unavailable, or prohibitively expensive if used
   per-service
2. Pod and ClusterIP CIDR blocks are identical on all clusters
3. Users do not have permissions to edit public DNS records
4. Clusters do not follow the rules of [Namespace
   Sameness](https://groups.google.com/forum/#!msg/kubernetes-sig-multicluster/jfDAMxFWlOg/9Z9O0mVpAgAJ)

## Walk Through

The intention of this section is give a high level explanation of the workings
of Multi-cluster DNS.

This walk through assumes:

- A [management
  cluster](https://cluster-api.sigs.k8s.io/reference/glossary.html#management-cluster)
  exists, running the Cluster API.

- Two or more workload Clusters have been deployed from the management cluster. For the sake
  of this doc, assume `cluster-a` and `cluster-b` exist. Both of these Clusters
  belong to the `dev-team` namespace on the management cluster.

The Multi-cluster DNS project installs a `capi-dns-controller` and a
GatewayDNSRecord CRD on the management cluster. The GatewayDNSRecord is
reconciled by the `capi-dns-controller`. An example of this resource may look
like this:

```yaml
---
apiVersion: connectivity.tanzu.vmware.com/v1alpha1
kind: GatewayDNS
metadata:
  name: dev-team-gateway-dns
  namespace: dev-team
spec:
  clusterSelector:
    matchLabels:
      hasContour: "true"
  service: projectcontour/envoy
  resolutionType: loadBalancer
```

This resource is applied by a member of a team that has access to the `dev-team`
namespace on the management cluster. The spec of this resource specifies a
`clusterSelector` that tells the reconciler which clusters shall be monitored
for gateways with a service namespaced/named `projectcontour/envoy` and has
`loadBalancer` resolution type.

In this example, let's assume `cluster-a` Cluster resource exists
on the management cluster, and has the label `hasContour: true`. The reconciler
would find `cluster-a` because it is in the correct namespace, has the desired
label. Then the reconciler connects to `cluster-a`'s API and search for a
service that is namedspace/named `projectcontour/envoy`, and has a service
resolution type `loadBalancer`. The reconciler collects the service's load
balanced IP.

The `capi-dns-controller` will match on `cluster-a` management cluster resource,
connects to  API and extracts the load balanced IP of contour. The controller
then distributes EndpointSlices to all of the clusters in the `dev-team`
management cluster namespace. In this example the reconciler connects to both
`cluster-a` and `cluster-b` APIs and applies EnspointSlice resources into the
`capi-dns` namespace that look like this:


```yaml
---
apiVersion: discovery.k8s.io/v1beta1
kind: EndpointSlice
metadata:
  name: foo-strawberry-gateway
  namespace: capi-dns
  annotations:
    connectivity.tanzu.vmware.com/dns-hostname: "*.gateway.strawberry.dev-team.clusters.xcc.test"
addressType: IPv4
endpoints:
- addresses:
  - "10.4.5.6"
```


The EndpointSlices have a fqdn annotation and the loadBalancer IP of the
discovered service. The fqdns are generated by the reconciler and have the
following format

  `*.gateway.<Cluster name>.<Cluster namespace>.clusters.<DOMAIN-SUFFIX>`

where the 'Cluster name' and 'Cluster namespace' are the namespace and name
of the Cluster resource on the management cluster.


Workload clusters run a second controller defined by this project, the
`capi-dns-server`. This controller exposes a DNS server that resolves addresses
in the configured DOMAIN-SUFFIX zone. In this example the `capi-dns-server`
serves for the `xcc.test` zone. The `capi-dns-server` serves A records that are
derived from the EndpointSlices. The cluster DNS server, `kube-dns`, is
configured to forward requests to the `capi-dns-server` in the `xcc.test` zone.



For this example let's also assume that nginx is deployed on `cluster-a`, served
by contour. An contour HTTPProxy resource configured with a virtualhost fqdn of
`nginx.gateway.cluster-a.dev-team.clusters.xcc.test`.









## Contributing

Please read [CONTRIBUTING.md](./CONTRIBUTING.md) for details on the process for
running tests, making changes, and submitting issues and pull requests to the
project.

## Code of Conduct

Please familiarize yourself with the [Code of Conduct](./CODE_OF_CONDUCT.md)
before contributing. This code of conduct applies to the **Cross-cluster
Connectivity** community at large (Slack, mailing lists, Twitter, etc.).

## License

This project is licensed under the Apache-2.0 License - see the
[LICENSE](LICENSE) file for details.
